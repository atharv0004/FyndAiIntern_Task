{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44a024f",
   "metadata": {},
   "source": [
    "## Step 1: Initial Experiment with Basic Prompts\n",
    "**Description:** This step sets up the environment, imports necessary libraries (Groq, Pandas, VaderSentiment), and defines four initial prompt strategies: Direct, Few-Shot, Chain-of-Thought (CoT), and Hybrid (combining Vader sentiment scores). It runs a pilot experiment on 50 sampled reviews from 'Book1.csv'.\n",
    "\n",
    "**Result:** The Direct and Few-Shot strategies tied for the best performance with 70% accuracy, while the Hybrid approach lagged behind at 54%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f754f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direct: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:19<00:00,  1.58s/it]\n",
      "fewshot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:45<00:00,  3.32s/it]\n",
      "cot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:35<00:00,  3.11s/it]\n",
      "hybrid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:23<00:00,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL RESULTS =====\n",
      "{\n",
      "    \"direct\": {\n",
      "        \"accuracy\": 0.7,\n",
      "        \"json_validity\": 1.0\n",
      "    },\n",
      "    \"fewshot\": {\n",
      "        \"accuracy\": 0.7,\n",
      "        \"json_validity\": 0.98\n",
      "    },\n",
      "    \"cot\": {\n",
      "        \"accuracy\": 0.64,\n",
      "        \"json_validity\": 1.0\n",
      "    },\n",
      "    \"hybrid\": {\n",
      "        \"accuracy\": 0.54,\n",
      "        \"json_validity\": 1.0\n",
      "    }\n",
      "}\n",
      "\n",
      "Saved predictions ‚Üí predicted_results1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# -----------------------------\n",
    "# SENTIMENT PREPROCESSOR\n",
    "# -----------------------------\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]  \n",
    "    label = (\n",
    "        \"very_negative\" if score <= -0.5 else\n",
    "        \"negative\" if score < 0 else\n",
    "        \"neutral\" if score < 0.3 else\n",
    "        \"positive\" if score < 0.6 else\n",
    "        \"very_positive\"\n",
    "    )\n",
    "    return score, label\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# API KEY + CLIENT\n",
    "# -----------------------------\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PROMPT TEMPLATES\n",
    "# -----------------------------\n",
    "def direct_prompt(review):\n",
    "    return f\"\"\"\n",
    "You are a Yelp rating classifier.\n",
    "Given a review, output JSON ONLY:\n",
    "\n",
    "{{\n",
    " \"predicted_stars\": <1-5 integer>,\n",
    " \"explanation\": \"<brief reasoning>\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def few_shot_prompt(review):\n",
    "    return f\"\"\"\n",
    "You are a Yelp rating classifier. Learn from examples:\n",
    "\n",
    "Example 1:\n",
    "Review: \"Amazing food and super friendly people!\"\n",
    "Output: {{\"predicted_stars\": 5, \"explanation\": \"Strong positive sentiment.\"}}\n",
    "\n",
    "Example 2:\n",
    "Review: \"Cold food. Slow service.\"\n",
    "Output: {{\"predicted_stars\": 2, \"explanation\": \"Mostly negative.\"}}\n",
    "\n",
    "Example 3:\n",
    "Review: \"It's fine. Not great, not terrible.\"\n",
    "Output: {{\"predicted_stars\": 3, \"explanation\": \"Neutral experience.\"}}\n",
    "\n",
    "Now classify the following:\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\n",
    "Return JSON ONLY.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def chain_of_thought_prompt(review):\n",
    "    return f\"\"\"\n",
    "You classify Yelp reviews into star ratings (1‚Äì5).\n",
    "\n",
    "INTERNAL RULES (Do NOT reveal reasoning):\n",
    "- 1 = extremely negative\n",
    "- 2 = mostly negative\n",
    "- 3 = mixed/average\n",
    "- 4 = mostly positive\n",
    "- 5 = extremely positive\n",
    "\n",
    "Think step-by-step internally but DO NOT show the steps.\n",
    "\n",
    "Output ONLY JSON:\n",
    "\n",
    "{{\n",
    " \"predicted_stars\": <1-5>,\n",
    " \"explanation\": \"<brief summary>\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def hybrid_prompt(review, sentiment_score, sentiment_label):\n",
    "    return f\"\"\"\n",
    "You are a Yelp star-rating classifier.\n",
    "\n",
    "You will receive:\n",
    "1. The original review text.\n",
    "2. A precomputed sentiment score from a rule-based analyzer.\n",
    "3. A sentiment label based on thresholds.\n",
    "\n",
    "Use BOTH the metadata AND the review to determine the final star rating (1‚Äì5).\n",
    "\n",
    "Mapping rules:\n",
    "- Very negative sentiment ‚Üí 1 or 2\n",
    "- Mixed or neutral ‚Üí 3\n",
    "- Mostly positive ‚Üí 4\n",
    "- Very positive ‚Üí 5\n",
    "\n",
    "Now produce JSON ONLY:\n",
    "\n",
    "{{\n",
    " \"predicted_stars\": <1-5>,\n",
    " \"explanation\": \"<reason considering review + sentiment metadata>\"\n",
    "}}\n",
    "\n",
    "Review Text:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\n",
    "Rule-Based Metadata:\n",
    "- sentiment_score: {sentiment_score}\n",
    "- sentiment_label: \"{sentiment_label}\"\n",
    "\n",
    "Return JSON only.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL CALL\n",
    "# -----------------------------\n",
    "def call_llama(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# JSON PARSER\n",
    "# -----------------------------\n",
    "def parse_json(output):\n",
    "    try:\n",
    "        data = json.loads(output)\n",
    "        return int(data[\"predicted_stars\"]), True\n",
    "    except:\n",
    "        return None, False\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# EXPERIMENT RUNNER\n",
    "# -----------------------------\n",
    "def run_experiment(df, prompt_type):\n",
    "    preds = []\n",
    "    json_valid_count = 0\n",
    "\n",
    "    for review in tqdm(df[\"text\"], desc=prompt_type):\n",
    "\n",
    "        if prompt_type == \"direct\":\n",
    "            prompt = direct_prompt(review)\n",
    "\n",
    "        elif prompt_type == \"fewshot\":\n",
    "            prompt = few_shot_prompt(review)\n",
    "\n",
    "        elif prompt_type == \"cot\":\n",
    "            prompt = chain_of_thought_prompt(review)\n",
    "\n",
    "        elif prompt_type == \"hybrid\":\n",
    "            score, label = preprocess_sentiment(review)\n",
    "            prompt = hybrid_prompt(review, score, label)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid prompt type\")\n",
    "\n",
    "        raw_output = call_llama(prompt)\n",
    "        stars, valid = parse_json(raw_output)\n",
    "\n",
    "        if valid and stars is not None:\n",
    "            preds.append(stars)\n",
    "            json_valid_count += 1\n",
    "        else:\n",
    "            preds.append(3)  # fallback\n",
    "\n",
    "    accuracy = accuracy_score(df[\"stars\"], preds)\n",
    "    json_validity = json_valid_count / len(df)\n",
    "    return preds, accuracy, json_validity\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN EXECUTION\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"./Book1.csv\")\n",
    "    df = df.sample(50, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for style in [\"direct\", \"fewshot\", \"cot\", \"hybrid\"]:\n",
    "        preds, acc, json_rate = run_experiment(df, style)\n",
    "\n",
    "        results[style] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"json_validity\": json_rate\n",
    "        }\n",
    "\n",
    "        df[f\"pred_{style}\"] = preds\n",
    "\n",
    "    print(\"\\n===== FINAL RESULTS =====\")\n",
    "    print(json.dumps(results, indent=4))\n",
    "\n",
    "    df.to_csv(\"predicted_results1.csv\", index=False)\n",
    "    print(\"\\nSaved predictions ‚Üí predicted_results1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740dda4",
   "metadata": {},
   "source": [
    "## Step 2: Refined Strategies and Ensembles (Small Scale)\n",
    "**Description:** This step introduces more sophisticated prompting techniques: Structured Analysis (breaking down components), Numerical Reasoning (scoring dimensions), and Contrastive (positives vs negatives). It also implements Ensemble methods (Majority Vote and Weighted) and runs a test on 20 reviews from 'yelp.csv'.\n",
    "\n",
    "**Result:** Structured Analysis and Ensemble methods achieved the highest accuracy of 75%, showing a 15.4% improvement over the Direct baseline on this small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccbd5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Available models: ['llama-3.1-8b-instant', 'llama-3.3-70b-versatile']\n",
      "‚úì Using: llama-3.1-8b-instant\n",
      "\n",
      "\n",
      "======================================================================\n",
      "YELP REVIEW CLASSIFIER - Multi-Prompt Ensemble\n",
      "======================================================================\n",
      "\n",
      "Loading data...\n",
      "Loaded 20 reviews\n",
      "\n",
      "Running experiments on 20 reviews...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ñ∂ Testing DIRECT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.6500 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "‚ñ∂ Testing STRUCTURED...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.7500 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "‚ñ∂ Testing NUMERICAL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.7000 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "‚ñ∂ Testing CONTRASTIVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.7000 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "======================================================================\n",
      "INDIVIDUAL MODEL RESULTS\n",
      "======================================================================\n",
      "1. structured      | Accuracy: 0.7500 | +15.4% vs baseline\n",
      "2. numerical       | Accuracy: 0.7000 | +7.7% vs baseline\n",
      "3. contrastive     | Accuracy: 0.7000 | +7.7% vs baseline\n",
      "4. direct          | Accuracy: 0.6500 | +0.0% vs baseline\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE STRATEGIES\n",
      "======================================================================\n",
      "\n",
      "‚ñ∂ MAJORITY VOTE ENSEMBLE...\n",
      "  ‚úì Accuracy: 0.7500\n",
      "\n",
      "‚ñ∂ WEIGHTED ENSEMBLE (by accuracy)...\n",
      "  ‚úì Accuracy: 0.7500\n",
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "ü•á 1. structured           | Accuracy: 0.7500 | +15.4%\n",
      "ü•à 2. ensemble_majority    | Accuracy: 0.7500 | +15.4%\n",
      "ü•â 3. ensemble_weighted    | Accuracy: 0.7500 | +15.4%\n",
      "   4. numerical            | Accuracy: 0.7000 | +7.7%\n",
      "   5. contrastive          | Accuracy: 0.7000 | +7.7%\n",
      "   6. direct               | Accuracy: 0.6500 | +0.0%\n",
      "\n",
      "======================================================================\n",
      "üèÜ OVERALL WINNER: STRUCTURED\n",
      "   Final Accuracy: 0.7500 (+15.4%)\n",
      "======================================================================\n",
      "\n",
      "‚úì Saved predictions ‚Üí predicted_results_final.csv\n",
      "‚úì Saved summary ‚Üí results_summary.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# API KEY + CLIENT\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "# ===== PROMPTS =====\n",
    "def prompt_direct(review):\n",
    "    \"\"\"Direct approach baseline\"\"\"\n",
    "    return f\"\"\"Rate as 1-5 stars. JSON ONLY: {{\"predicted_stars\": <1-5>}}\n",
    "Review: \"{review}\\\"\"\"\"\n",
    "\n",
    "def prompt_structured_analysis(review):\n",
    "    \"\"\"Breaks down review into specific components\"\"\"\n",
    "    return f\"\"\"\n",
    "You are a Yelp rating expert. Analyze this review COMPONENT BY COMPONENT:\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Output ONLY valid JSON with this exact structure:\n",
    "{{\n",
    " \"service_quality\": \"positive/negative/neutral/not_mentioned\",\n",
    " \"food_quality\": \"positive/negative/neutral/not_mentioned\",\n",
    " \"ambiance\": \"positive/negative/neutral/not_mentioned\",\n",
    " \"value\": \"positive/negative/neutral/not_mentioned\",\n",
    " \"overall_sentiment\": \"very_negative/negative/neutral/positive/very_positive\",\n",
    " \"predicted_stars\": <1-5>,\n",
    " \"explanation\": \"<one sentence>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt_numerical_reasoning(review):\n",
    "    \"\"\"Uses numerical scoring before mapping to stars\"\"\"\n",
    "    return f\"\"\"\n",
    "You are a Yelp classifier. Score each dimension 0-10, then convert to stars.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "Scoring (0-10 for each):\n",
    "- Satisfaction level: ___\n",
    "- Would recommend: ___\n",
    "- Problem severity (inverse): ___\n",
    "\n",
    "Output ONLY valid JSON:\n",
    "{{\n",
    " \"satisfaction_score\": <0-10>,\n",
    " \"recommendation_score\": <0-10>,\n",
    " \"problem_severity\": <0-10>,\n",
    " \"predicted_stars\": <1-5>,\n",
    " \"explanation\": \"<reasoning>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt_contrastive(review):\n",
    "    \"\"\"Explicitly compares positive and negative aspects\"\"\"\n",
    "    return f\"\"\"\n",
    "You are a Yelp rater. Compare positives vs negatives.\n",
    "\n",
    "Review: \"{review}\"\n",
    "\n",
    "List main positives (if any):\n",
    "- [positive aspects]\n",
    "\n",
    "List main negatives (if any):\n",
    "- [negative aspects]\n",
    "\n",
    "Weighting: Which dominates?\n",
    "\n",
    "Output ONLY valid JSON:\n",
    "{{\n",
    " \"positive_count\": <number>,\n",
    " \"negative_count\": <number>,\n",
    " \"dominant_sentiment\": \"positive/negative/neutral\",\n",
    " \"predicted_stars\": <1-5>,\n",
    " \"explanation\": \"<comparison-based reasoning>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_available_model():\n",
    "    \"\"\"Get first available text model from Groq\"\"\"\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        # Filter for actual chat models (exclude guards, prompts guards, etc)\n",
    "        text_models = [\n",
    "            m.id for m in models.data \n",
    "            if any(x in m.id.lower() for x in ['llama-3.3', 'llama-3.1', 'mixtral'])\n",
    "            and 'guard' not in m.id.lower()\n",
    "            and 'prompt' not in m.id.lower()\n",
    "        ]\n",
    "        if text_models:\n",
    "            print(f\"\\n‚úì Available models: {text_models}\")\n",
    "            model_choice = text_models[0]\n",
    "            print(f\"‚úì Using: {model_choice}\\n\")\n",
    "            return model_choice\n",
    "        else:\n",
    "            print(f\"\\n‚úì Using fallback: llama-3.3-70b-versatile\\n\")\n",
    "            return \"llama-3.3-70b-versatile\"\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úì Using fallback: llama-3.3-70b-versatile\\n\")\n",
    "        return \"llama-3.3-70b-versatile\"\n",
    "\n",
    "GROQ_MODEL = get_available_model()\n",
    "\n",
    "# MODEL CALL with retry logic\n",
    "def call_llama(prompt, model=None, max_retries=3):\n",
    "    \"\"\"Call Groq API with retry logic and throttling.\"\"\"\n",
    "    if model is None:\n",
    "        model = GROQ_MODEL\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            time.sleep(0.3)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower() and attempt < max_retries - 1:\n",
    "                wait_time = (2 ** attempt) * 3\n",
    "                tqdm.write(f\"\\nRate limit. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "# JSON PARSER\n",
    "def parse_json(output):\n",
    "    try:\n",
    "        data = json.loads(output)\n",
    "        stars = int(data.get(\"predicted_stars\"))\n",
    "        if 1 <= stars <= 5:\n",
    "            return stars, True\n",
    "        return None, False\n",
    "    except:\n",
    "        return None, False\n",
    "\n",
    "\n",
    "# EXPERIMENT RUNNER\n",
    "def run_experiment(df, prompt_type):\n",
    "    preds = []\n",
    "    json_valid_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    prompt_map = {\n",
    "        \"direct\": prompt_direct,\n",
    "        \"structured\": prompt_structured_analysis,\n",
    "        \"numerical\": prompt_numerical_reasoning,\n",
    "        \"contrastive\": prompt_contrastive\n",
    "    }\n",
    "    \n",
    "    prompt_fn = prompt_map.get(prompt_type)\n",
    "    if not prompt_fn:\n",
    "        raise ValueError(f\"Invalid prompt type: {prompt_type}\")\n",
    "\n",
    "    for review in tqdm(df[\"text\"], desc=prompt_type, leave=False):\n",
    "        try:\n",
    "            prompt = prompt_fn(review)\n",
    "            raw = call_llama(prompt)\n",
    "            stars, valid = parse_json(raw)\n",
    "\n",
    "            if valid and stars is not None:\n",
    "                preds.append(stars)\n",
    "                json_valid_count += 1\n",
    "            else:\n",
    "                preds.append(3)\n",
    "                failed_count += 1\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"\\nError: {str(e)[:80]}\")\n",
    "            preds.append(3)\n",
    "            failed_count += 1\n",
    "\n",
    "    accuracy = accuracy_score(df[\"stars\"], preds)\n",
    "    json_validity = json_valid_count / len(df)\n",
    "    return preds, accuracy, json_validity, failed_count\n",
    "\n",
    "\n",
    "# ENSEMBLE FUNCTIONS\n",
    "def create_ensemble(individual_preds_dict):\n",
    "    \"\"\"Majority vote ensemble\"\"\"\n",
    "    n = len(next(iter(individual_preds_dict.values())))\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    for idx in range(n):\n",
    "        votes = [individual_preds_dict[model][idx] for model in sorted(individual_preds_dict.keys())]\n",
    "        consensus = int(np.median(votes))\n",
    "        ensemble_preds.append(consensus)\n",
    "    \n",
    "    return ensemble_preds\n",
    "\n",
    "\n",
    "def create_weighted_ensemble(individual_preds_dict, weights):\n",
    "    \"\"\"Weighted ensemble based on accuracy\"\"\"\n",
    "    n = len(next(iter(individual_preds_dict.values())))\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    for idx in range(n):\n",
    "        weighted_sum = sum(individual_preds_dict[model][idx] * weights[model] \n",
    "                          for model in sorted(individual_preds_dict.keys()))\n",
    "        weighted_avg = weighted_sum / sum(weights.values())\n",
    "        ensemble_preds.append(round(weighted_avg))\n",
    "    \n",
    "    return ensemble_preds\n",
    "\n",
    "\n",
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"YELP REVIEW CLASSIFIER - Multi-Prompt Ensemble\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nLoading data...\")\n",
    "    df = pd.read_csv(\"./yelp.csv\")\n",
    "    df = df.sample(20, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} reviews\")\n",
    "\n",
    "    results = {}\n",
    "    individual_preds = {}\n",
    "    \n",
    "    prompt_styles = [\"direct\",\"structured\", \"numerical\", \"contrastive\"]\n",
    "\n",
    "    print(f\"\\nRunning experiments on {len(df)} reviews...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "\n",
    "    for style in prompt_styles:\n",
    "        print(f\"\\n‚ñ∂ Testing {style.upper()}...\")\n",
    "        preds, acc, json_rate, failed = run_experiment(df, style)\n",
    "        results[style] = {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"json_validity\": round(json_rate, 4),\n",
    "            \"failed_parses\": failed\n",
    "        }\n",
    "        individual_preds[style] = preds\n",
    "        df[f\"pred_{style}\"] = preds\n",
    "        print(f\"  ‚úì Accuracy: {acc:.4f} | JSON Valid: {json_rate:.4f} | Failed: {failed}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INDIVIDUAL MODEL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1][\"accuracy\"], reverse=True)\n",
    "    for i, (style, metrics) in enumerate(sorted_results, 1):\n",
    "        acc = metrics[\"accuracy\"]\n",
    "        improvement = ((acc - 0.650) / 0.650) * 100\n",
    "        print(f\"{i}. {style:15} | Accuracy: {acc:.4f} | {improvement:+.1f}% vs baseline\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENSEMBLE STRATEGIES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Majority Vote Ensemble\n",
    "    print(\"\\n‚ñ∂ MAJORITY VOTE ENSEMBLE...\")\n",
    "    ensemble_majority = create_ensemble(individual_preds)\n",
    "    ensemble_majority_acc = accuracy_score(df[\"stars\"], ensemble_majority)\n",
    "    df[\"pred_ensemble_majority\"] = ensemble_majority\n",
    "    print(f\"  ‚úì Accuracy: {ensemble_majority_acc:.4f}\")\n",
    "\n",
    "    # Weighted Ensemble\n",
    "    print(\"\\n‚ñ∂ WEIGHTED ENSEMBLE (by accuracy)...\")\n",
    "    weights = {k: v[\"accuracy\"] for k, v in results.items()}\n",
    "    ensemble_weighted = create_weighted_ensemble(individual_preds, weights)\n",
    "    ensemble_weighted_acc = accuracy_score(df[\"stars\"], ensemble_weighted)\n",
    "    df[\"pred_ensemble_weighted\"] = ensemble_weighted\n",
    "    print(f\"  ‚úì Accuracy: {ensemble_weighted_acc:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    all_results = {\n",
    "        **results,\n",
    "        \"ensemble_majority\": {\"accuracy\": round(ensemble_majority_acc, 4)},\n",
    "        \"ensemble_weighted\": {\"accuracy\": round(ensemble_weighted_acc, 4)}\n",
    "    }\n",
    "\n",
    "    final_sorted = sorted(all_results.items(), key=lambda x: x[1][\"accuracy\"], reverse=True)\n",
    "    \n",
    "    for i, (name, metrics) in enumerate(final_sorted, 1):\n",
    "        acc = metrics[\"accuracy\"]\n",
    "        improvement = ((acc - 0.650) / 0.650) * 100\n",
    "        medal = [\"ü•á\", \"ü•à\", \"ü•â\"][i-1] if i <= 3 else \"  \"\n",
    "        print(f\"{medal} {i}. {name:20} | Accuracy: {acc:.4f} | {improvement:+.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    winner = final_sorted[0]\n",
    "    improvement_pct = ((winner[1]['accuracy'] - 0.650) / 0.650) * 100\n",
    "    print(f\"üèÜ OVERALL WINNER: {winner[0].upper()}\")\n",
    "    print(f\"   Final Accuracy: {winner[1]['accuracy']:.4f} (+{improvement_pct:.1f}%)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Save outputs\n",
    "    df.to_csv(\"predicted_results_final.csv\", index=False)\n",
    "    with open(\"results_summary.json\", \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úì Saved predictions ‚Üí predicted_results_final.csv\")\n",
    "    print(f\"‚úì Saved summary ‚Üí results_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756085af",
   "metadata": {},
   "source": [
    "## Step 3: Large Scale Validation (200 Reviews)\n",
    "**Description:** This step scales the experiment from Step 2 to 200 randomly sampled reviews to test the robustness of the strategies and ensembles on a larger dataset.\n",
    "\n",
    "**Result:** The simple Direct approach surprisingly performed best with 67.5% accuracy, outperforming the more complex Structured and Ensemble methods on the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dca8af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "YELP REVIEW CLASSIFIER - Multi-Prompt Ensemble\n",
      "======================================================================\n",
      "\n",
      "Loading data...\n",
      "Loaded 200 reviews\n",
      "\n",
      "Running experiments on 200 reviews...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ñ∂ Testing DIRECT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.6750 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "‚ñ∂ Testing STRUCTURED...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.6700 | JSON Valid: 0.9950 | Failed: 1\n",
      "\n",
      "‚ñ∂ Testing NUMERICAL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.6200 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "‚ñ∂ Testing CONTRASTIVE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Accuracy: 0.5850 | JSON Valid: 1.0000 | Failed: 0\n",
      "\n",
      "======================================================================\n",
      "INDIVIDUAL MODEL RESULTS\n",
      "======================================================================\n",
      "1. direct          | Accuracy: 0.6750 | +3.8% vs baseline\n",
      "2. structured      | Accuracy: 0.6700 | +3.1% vs baseline\n",
      "3. numerical       | Accuracy: 0.6200 | -4.6% vs baseline\n",
      "4. contrastive     | Accuracy: 0.5850 | -10.0% vs baseline\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE STRATEGIES\n",
      "======================================================================\n",
      "\n",
      "‚ñ∂ MAJORITY VOTE ENSEMBLE...\n",
      "  ‚úì Accuracy: 0.6700\n",
      "\n",
      "‚ñ∂ WEIGHTED ENSEMBLE (by accuracy)...\n",
      "  ‚úì Accuracy: 0.6700\n",
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "ü•á 1. direct               | Accuracy: 0.6750 | +3.8%\n",
      "ü•à 2. structured           | Accuracy: 0.6700 | +3.1%\n",
      "ü•â 3. ensemble_majority    | Accuracy: 0.6700 | +3.1%\n",
      "   4. ensemble_weighted    | Accuracy: 0.6700 | +3.1%\n",
      "   5. numerical            | Accuracy: 0.6200 | -4.6%\n",
      "   6. contrastive          | Accuracy: 0.5850 | -10.0%\n",
      "\n",
      "======================================================================\n",
      "üèÜ OVERALL WINNER: DIRECT\n",
      "   Final Accuracy: 0.6750 (+3.8%)\n",
      "======================================================================\n",
      "\n",
      "‚úì Saved predictions ‚Üí predicted_results_final.csv\n",
      "‚úì Saved summary ‚Üí results_summary.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"YELP REVIEW CLASSIFIER - Multi-Prompt Ensemble\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nLoading data...\")\n",
    "    df = pd.read_csv(\"./yelp.csv\")\n",
    "    df = df.sample(200, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} reviews\")\n",
    "\n",
    "    results = {}\n",
    "    individual_preds = {}\n",
    "    \n",
    "    prompt_styles = [\"direct\",\"structured\", \"numerical\", \"contrastive\"]\n",
    "\n",
    "    print(f\"\\nRunning experiments on {len(df)} reviews...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "\n",
    "    for style in prompt_styles:\n",
    "        print(f\"\\n‚ñ∂ Testing {style.upper()}...\")\n",
    "        preds, acc, json_rate, failed = run_experiment(df, style)\n",
    "        results[style] = {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"json_validity\": round(json_rate, 4),\n",
    "            \"failed_parses\": failed\n",
    "        }\n",
    "        individual_preds[style] = preds\n",
    "        df[f\"pred_{style}\"] = preds\n",
    "        print(f\"  ‚úì Accuracy: {acc:.4f} | JSON Valid: {json_rate:.4f} | Failed: {failed}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INDIVIDUAL MODEL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1][\"accuracy\"], reverse=True)\n",
    "    for i, (style, metrics) in enumerate(sorted_results, 1):\n",
    "        acc = metrics[\"accuracy\"]\n",
    "        improvement = ((acc - 0.650) / 0.650) * 100\n",
    "        print(f\"{i}. {style:15} | Accuracy: {acc:.4f} | {improvement:+.1f}% vs baseline\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENSEMBLE STRATEGIES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Majority Vote Ensemble\n",
    "    print(\"\\n‚ñ∂ MAJORITY VOTE ENSEMBLE...\")\n",
    "    ensemble_majority = create_ensemble(individual_preds)\n",
    "    ensemble_majority_acc = accuracy_score(df[\"stars\"], ensemble_majority)\n",
    "    df[\"pred_ensemble_majority\"] = ensemble_majority\n",
    "    print(f\"  ‚úì Accuracy: {ensemble_majority_acc:.4f}\")\n",
    "\n",
    "    # Weighted Ensemble\n",
    "    print(\"\\n‚ñ∂ WEIGHTED ENSEMBLE (by accuracy)...\")\n",
    "    weights = {k: v[\"accuracy\"] for k, v in results.items()}\n",
    "    ensemble_weighted = create_weighted_ensemble(individual_preds, weights)\n",
    "    ensemble_weighted_acc = accuracy_score(df[\"stars\"], ensemble_weighted)\n",
    "    df[\"pred_ensemble_weighted\"] = ensemble_weighted\n",
    "    print(f\"  ‚úì Accuracy: {ensemble_weighted_acc:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    all_results = {\n",
    "        **results,\n",
    "        \"ensemble_majority\": {\"accuracy\": round(ensemble_majority_acc, 4)},\n",
    "        \"ensemble_weighted\": {\"accuracy\": round(ensemble_weighted_acc, 4)}\n",
    "    }\n",
    "\n",
    "    final_sorted = sorted(all_results.items(), key=lambda x: x[1][\"accuracy\"], reverse=True)\n",
    "    \n",
    "    for i, (name, metrics) in enumerate(final_sorted, 1):\n",
    "        acc = metrics[\"accuracy\"]\n",
    "        improvement = ((acc - 0.650) / 0.650) * 100\n",
    "        medal = [\"ü•á\", \"ü•à\", \"ü•â\"][i-1] if i <= 3 else \"  \"\n",
    "        print(f\"{medal} {i}. {name:20} | Accuracy: {acc:.4f} | {improvement:+.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    winner = final_sorted[0]\n",
    "    improvement_pct = ((winner[1]['accuracy'] - 0.650) / 0.650) * 100\n",
    "    print(f\"üèÜ OVERALL WINNER: {winner[0].upper()}\")\n",
    "    print(f\"   Final Accuracy: {winner[1]['accuracy']:.4f} (+{improvement_pct:.1f}%)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Save outputs\n",
    "    df.to_csv(\"predicted_results_final_200.csv\", index=False)\n",
    "    with open(\"results_summary_final.json\", \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úì Saved predictions ‚Üí predicted_results_final.csv\")\n",
    "    print(f\"‚úì Saved summary ‚Üí results_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
